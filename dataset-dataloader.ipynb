{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPgcMPoB38FBjxOmAraAHsi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Assumptions made:\n","#### 1.) Images and corresponding label files have the same filename.\n","#### 2.) Images file extention(s) are => .png, .jpg\n","#### 3.) Label file extention(s) are => .txt\n","#### 4.) Record in label file format=> [class_id, xmin, ymin, xmax, ymax]\n","#### 5.) class_id is an integer and NOT vehicle-type in string\n","#### 6.) Torch tensor of labels dtype is 64-bit ints and torch 2-D tensor of bounding boxes dtype is 32-bit floats. Precision can be reduced for speed"],"metadata":{"id":"XVQhzbChmRjW"}},{"cell_type":"markdown","source":["## Imports"],"metadata":{"id":"--7JZQ52lnQf"}},{"cell_type":"code","source":["import os\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from PIL import Image\n","import torchvision.transforms as T"],"metadata":{"id":"7cNCs1OVllu_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dataset and Dataloader"],"metadata":{"id":"_UufR8xKkaZZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"eWVETZuMkZX5"},"outputs":[],"source":["class VehicleDataset(Dataset):\n","    def __init__(self, image_dir, label_dir, transform=None):\n","        self.image_dir = image_dir\n","        self.label_dir = label_dir\n","        self.transform = transform\n","\n","        self.image_filenames = sorted([\n","            fname for fname in os.listdir(image_dir) if fname.endswith(('.jpg', '.png'))\n","        ])\n","\n","    def __len__(self):\n","        return len(self.image_filenames)\n","\n","    def __getitem__(self, idx):\n","        # Process Image\n","        img_name = self.image_filenames[idx]\n","        img_path = os.path.join(self.image_dir, img_name)\n","        image = Image.open(img_path).convert(\"RGB\")\n","        W, H = image.size\n","\n","        # Process label(s) - (1) vehicle class and (2) bounding box coordinates\n","        label_path = os.path.join(self.label_dir, img_name.replace('.jpg', '.txt').replace('.png', '.txt'))\n","        labels = []\n","        boxes = []\n","        with open(label_path, 'r') as f:\n","            for line in f.readlines():\n","                parts = line.strip().split()\n","\n","                class_id = int(parts[0])\n","                xmin, ymin, xmax, ymax = map(float, parts[1:])\n","\n","                labels.append(class_id)\n","                boxes.append([xmin, ymin, xmax, ymax])\n","\n","        # Python list -> PyTorch tensor\n","        boxes = torch.tensor(boxes, dtype=torch.float32)\n","        labels = torch.tensor(labels, dtype=torch.int64)\n","\n","        target = {\n","            \"labels\": labels,\n","            \"boxes\": boxes,\n","            # \"image_id\": torch.tensor([idx])\n","        }\n","\n","        # Apply any transforms: resizing (if required) and converting to PyTorch tensor\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        # Return format: (<image>, {labels: tensor<1,2,...,n>, boxes: tensor<[tl_x, tl_y, br_x, br_y],...>})\n","        return image, target\n","\n","# Transform: (mostly not required to resize)\n","transform = T.Compose([\n","    T.Resize((1920, 1080)),\n","    T.ToTensor()\n","])\n","\n","# Instantiate dataset and dataloader pair(s)\n","dataset = VehicleDataset(image_dir=\"images\", label_dir=\"labels\", transform=transform)\n","dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True, collate_fn=lambda x: tuple(zip(*x)))"]},{"cell_type":"markdown","source":["## For understanding only"],"metadata":{"id":"g4D9Pdp0sP1w"}},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","\n","data = [(\"img1\", \"label1\"), (\"img2\", \"label2\"), (\"img3\", \"label3\"), (\"img4\", \"label4\")]\n","loader = DataLoader(data, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n","\n","for batch in loader:\n","    print(batch)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RCC4TMv2sPaQ","executionInfo":{"status":"ok","timestamp":1749875366007,"user_tz":-330,"elapsed":7,"user":{"displayName":"Raahul M","userId":"15449333048096671231"}},"outputId":"ba0b0bec-2eb6-435e-849e-ea8e9d721ece"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(('img3', 'img2'), ('label3', 'label2'))\n","(('img1', 'img4'), ('label1', 'label4'))\n"]}]}]}