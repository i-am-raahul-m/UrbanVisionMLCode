{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMLi+F0IUj84s21KTDTiUYX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Training"],"metadata":{"id":"D7wJfsFXzgVb"}},{"cell_type":"markdown","source":["## Installations"],"metadata":{"id":"MuyVrD-txeUM"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"xzdIHHe2tXmV","executionInfo":{"status":"ok","timestamp":1750093978752,"user_tz":-330,"elapsed":6462,"user":{"displayName":"Raahul M","userId":"15449333048096671231"}}},"outputs":[],"source":["# pip install transformers torchvision datasets evaluate matplotlib\n","! pip install -q evaluate"]},{"cell_type":"markdown","source":["## Id <-> label look-up maps"],"metadata":{"id":"sEmLLvaWxhI0"}},{"cell_type":"code","source":["id2label = {\n","    0: \"Auto\", 1: \"2-Wheeler\", 2: \"Bicycle\", 3: \"Bus\", 4: \"Hatchback\",\n","    5: \"LCV\", 6: \"Mini-bus\", 7: \"MUV\", 8: \"Sedan\", 9: \"SUV\",\n","    10: \"Tempo-traveller\", 11: \"Truck\", 12: \"Van\", 13: \"Vehicle (others)\"\n","}\n","label2id = {v: k for k, v in id2label.items()}"],"metadata":{"id":"r_jt-tKGtid9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Import the dataset"],"metadata":{"id":"PPgoR-qoxnNk"}},{"cell_type":"code","source":["from datasets import load_dataset\n","\n","dataset = load_dataset('imagefolder', data_dir='dataset/', split='train')"],"metadata":{"id":"_QzbQkZ2tlML"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load the base DETR model"],"metadata":{"id":"FVpzsodLxqEz"}},{"cell_type":"code","source":["from transformers import DetrImageProcessor, DetrForObjectDetection\n","\n","processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n","model = DetrForObjectDetection.from_pretrained(\n","    \"facebook/detr-resnet-50\",\n","    num_labels=14,\n","    ignore_mismatched_sizes=True,  # Important to allow changing number of classes\n","    id2label=id2label,\n","    label2id=label2id\n",")"],"metadata":{"id":"Y7HhQ_zUtnn1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Encode dataset images"],"metadata":{"id":"MASl_AvCxwBU"}},{"cell_type":"code","source":["def transform(example):\n","    image = example['image']\n","    annotations = {\n","        \"image_id\": example[\"image_id\"],\n","        \"annotations\": example[\"objects\"]  # Assumes 'objects' field is COCO-style\n","    }\n","    encoding = processor(images=image, annotations=annotations, return_tensors=\"pt\")\n","    encoding = {k: v.squeeze() for k, v in encoding.items()}  # remove batch dimension\n","    return encoding"],"metadata":{"id":"1PsXBA17tq9r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = dataset[\"train\"].map(transform, remove_columns=dataset[\"train\"].column_names)\n","val_dataset = dataset[\"val\"].map(transform, remove_columns=dataset[\"val\"].column_names)"],"metadata":{"id":"qJ6Vkqx7tup8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training parameters"],"metadata":{"id":"m4ApxNFWx7gU"}},{"cell_type":"code","source":["from transformers import Trainer, TrainingArguments\n","\n","args = TrainingArguments(\n","    output_dir=\"./detr-resnet-50-vehicle-finetuned\",\n","    evaluation_strategy=\"epoch\",\n","    per_device_train_batch_size=4,\n","    per_device_eval_batch_size=4,\n","    num_train_epochs=25,\n","    learning_rate=1e-5,\n","    save_strategy=\"epoch\",\n","    logging_dir=\"./logs\",\n","    logging_steps=10,\n","    save_total_limit=2,\n","    fp16=True\n",")"],"metadata":{"id":"LrF2UKxPyBL1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Training loop"],"metadata":{"id":"QEB19G23yDlE"}},{"cell_type":"code","source":["trainer = Trainer(\n","    model=model,\n","    args=args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    tokenizer=processor,  # needed for Trainer to call on batch\n",")"],"metadata":{"id":"hlU_zKnBtvg-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"ER7g04ciyH7F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Push to Huggingface"],"metadata":{"id":"7nwlz4f8yJFt"}},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","notebook_login()"],"metadata":{"id":"IB5P4iY0t5-C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.push_to_hub(\"Facebook's detection transformer architecture with resnet 50 supervised finetuned for detection and classification of vehicles\")"],"metadata":{"id":"U8nwGv0WuCAs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Inference"],"metadata":{"id":"TN77_LSozjYm"}},{"cell_type":"markdown","source":["## Load base model with lora weights"],"metadata":{"id":"AIpMuylDyO8F"}},{"cell_type":"code","source":["from transformers import DetrForObjectDetection, DetrImageProcessor\n","import torch\n","import requests\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","\n","model_name = \"xxx-i-am-raahul-m-xxx/detr-resnet-50-vehicle-finetuned\"\n","\n","model = DetrForObjectDetection.from_pretrained(model_name)\n","processor = DetrImageProcessor.from_pretrained(model_name)\n","model.eval()\n","model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"fwDFVHroysTb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Load the image to be inferenced"],"metadata":{"id":"wTqGwWWGzXOc"}},{"cell_type":"code","source":["# Load image\n","image_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n","image = Image.open(requests.get(image_url, stream=True).raw).convert(\"RGB\")\n","\n","# Preprocess\n","inputs = processor(images=image, return_tensors=\"pt\").to(model.device)\n","\n","# Predict\n","with torch.no_grad():\n","    outputs = model(**inputs)\n","\n","# Extract logits and boxes\n","logits = outputs.logits.softmax(-1)[0, :, :-1]  # exclude the \"no object\" class\n","boxes = outputs.pred_boxes[0]\n","\n","# Get top predictions\n","scores, labels = logits.max(-1)\n","keep = scores > 0.8  # threshold\n","scores = scores[keep]\n","labels = labels[keep]\n","boxes = boxes[keep]\n","\n","# Scale boxes to original image size\n","width, height = image.size\n","boxes = boxes * torch.tensor([width, height, width, height])\n","boxes = boxes.cpu().numpy()\n","labels = labels.cpu().numpy()\n","scores = scores.cpu().numpy()"],"metadata":{"id":"fVebtoCDxZss"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Visualization"],"metadata":{"id":"9AdE2T51zbMN"}},{"cell_type":"code","source":["plt.figure(figsize=(12, 8))\n","plt.imshow(image)\n","ax = plt.gca()\n","\n","for box, label, score in zip(boxes, labels, scores):\n","    x_c, y_c, w, h = box\n","    x0 = x_c - w / 2\n","    y0 = y_c - h / 2\n","\n","    rect = patches.Rectangle((x0, y0), w, h, linewidth=2, edgecolor='red', facecolor='none')\n","    ax.add_patch(rect)\n","\n","    class_name = model.config.id2label[label]\n","    ax.text(x0, y0, f\"{class_name}: {score:.2f}\", color=\"black\", fontsize=12,\n","            bbox=dict(facecolor=\"yellow\", alpha=0.5))\n","\n","plt.axis(\"off\")\n","plt.show()"],"metadata":{"id":"_u4qVXwAxcS0"},"execution_count":null,"outputs":[]}]}